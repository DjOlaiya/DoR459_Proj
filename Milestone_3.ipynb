{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from tqdm import tqdm #creates progress bar to let you know how long is left till function is complete\n",
    "import xgboost as xgb\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTENC\n",
    "from imblearn.pipeline import Pipeline\n",
    "from pandas.tseries.offsets import DateOffset"
   ]
  },
  {
   "source": [
    "# Original Processed Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv('/content/drive/MyDrive/DoR/cases_train_processed.csv')\n",
    "df = df.drop(['Unnamed: 0','index','source','additional_information','Last_Update','Lat_right','Long_right','Province_State','Country_Region','dist_between_in_km'],1)\n",
    "df['Confirmed'].fillna(df['Confirmed'].mean(),inplace=True)\n",
    "df['Deaths'].fillna(df.Deaths.mean(),inplace=True)\n",
    "df['Recovered'].fillna(df.Recovered.mean(),inplace=True)\n",
    "df['Active'].fillna(df.Active.mean(),inplace=True)\n",
    "df['Incidence_Rate'].fillna(df.Incidence_Rate.mean(),inplace=True)\n",
    "df['Case-Fatality_Ratio'].fillna(df['Case-Fatality_Ratio'].mean(),inplace=True)\n",
    "df.date_confirmation = pd.to_datetime(df.date_confirmation,infer_datetime_format=True) \n",
    "df.Combined_Key.fillna((df.province+\" ,\"+df.country),inplace=True)"
   ]
  },
  {
   "source": [
    "# Make 2 week Bins for date_confirmation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add bins to dataframe\n",
    "%%time\n",
    "dateBinDict = getDateBins(df.date_confirmation)\n",
    "df['date_labels'] = df.date_confirmation.apply(lambda curr_date : binDate(curr_date,dateBinDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDateBins(data):\n",
    "\n",
    "  \"\"\" funtion takes in the date column, creates an array of 2 week intervals\n",
    "   starting from the earliest day in the dataset\n",
    "    and returns a dictionary of ordinal data where the k is the new label\n",
    "    and value is the 2 week interval.\n",
    "    ex: earliest confrimed date is 2020-01-02 -> \n",
    "    interval is (2020-01-02, 2020-01-16). dict returns 1:(2020-01-02, 2020-01-16)\n",
    "  \"\"\"\n",
    "  start_date = data.min() - DateOffset(days=14)#earliest date + buffer\n",
    "  print(\"start date is \",start_date)\n",
    "  end_date = data.max() + DateOffset(days=14) #buffer to make sure nothing missed\n",
    "  print(\"end date is \", end_date)\n",
    "  date_interval = pd.interval_range(start=start_date,end=end_date,freq='14D')\n",
    "  date_dict = {}\n",
    "  for i in range(len(date_interval)):\n",
    "    date_dict[i+1] = date_interval[i]\n",
    "  return date_dict\n",
    "\n",
    "def binDate(row_date,ddict):\n",
    "  \"\"\" input is date_confrimation column. if date is in interval\n",
    "  returns the dictionary key\"\"\"\n",
    "  for k,v in ddict.items():\n",
    "    # if row_date in v:\n",
    "    #   print(\"here is key \",k)\n",
    "    #   print(\"here is interval\",v)\n",
    "    if row_date in v:\n",
    "      return k\n"
   ]
  },
  {
   "source": [
    "# Over and Undersampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops duplicated columns created\n",
    "def dropDuplicates(data):\n",
    "    duplicates = data.columns[data.columns.duplicated()]\n",
    "    if len(duplicates) > 0:\n",
    "        data = data.loc[:,~data.columns.duplicated()]\n",
    "    return data\n",
    "\n",
    "#drops all the columns not used in X_train\n",
    "def colsToDrop(dataframe):\n",
    "    if 'outcome' in dataframe.columns:\n",
    "        dataframe = dataframe.drop('outcome',1)\n",
    "        print(\"dropping the outcome column\")\n",
    "    if 'Combined_Key' in dataframe.columns:\n",
    "        dataframe = dataframe.drop('Combined_Key',1)\n",
    "        print(\"dropping Combined_Key\")\n",
    "    if 'dist_between_in_km' in dataframe.columns:\n",
    "        dataframe = dataframe.drop('dist_between_in_km',1)\n",
    "        print(\"dropping dist in km column\")\n",
    "    if 'date_confirmation' in dataframe.columns:\n",
    "        dataframe = dataframe.drop('date_confirmation',1)\n",
    "        print(\"dropping date\")\n",
    "    if 'date_labels' in dataframe.columns:\n",
    "        dataframe.date_labels = dataframe.date_labels.astype('object')\n",
    "        print(\"converting date to categorical\")\n",
    "    if 'age' in dataframe.columns:\n",
    "        dataframe.age = dataframe.age.astype('object')\n",
    "        print(\"converting age to categorical\")\n",
    "    return dataframe\n",
    "    \n",
    "    # once hot encode and add new cols to dataframe\n",
    "def oneHotEncode_df(dataframe):\n",
    "    dataframe = colsToDrop(dataframe)\n",
    "    col2Encode = list(dataframe.select_dtypes(include=['object'])) #gets a list of all the features that are objects assumption is that those are categorical\n",
    "    dummies = pd.get_dummies(dataframe,columns=col2Encode,prefix=col2Encode,sparse=True)\n",
    "    res = pd.concat([dataframe, dummies], axis=1)\n",
    "    #if we decide to drop one hot encoded values\n",
    "    res = res.drop(col2Encode, axis=1)\n",
    "    output = dropDuplicates(res)\n",
    "    return output\n",
    "\n",
    "    # SMOTENC needs a list of the indices of all the categorical variables in dataset\n",
    "    # ex: if country is column 2 and age is col 13. returns [2,13]\n",
    "def getCategoricalIndices(dataframe):\n",
    "    #get the indices of all the categorical variables\n",
    "    numericalVar = list(dataframe.select_dtypes(include=['float64']))\n",
    "    catlist = list(dataframe.columns.difference(numericalVar))\n",
    "    print(\"these are the categorical features: {}\".format(catlist))\n",
    "    indlist = []\n",
    "    for i in catlist:\n",
    "        indlist.append(dataframe.columns.get_loc(i))\n",
    "    return indlist"
   ]
  },
  {
   "source": [
    "## Code for  Creating OverSampled Data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Perform Oversampling before ohe.\n",
    "#so drop columns not used in independent var \n",
    "sparse_df = colsToDrop(df)\n",
    "#get list of categorical indices\n",
    "catIndList = getCategoricalIndices(sparse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#oversample \n",
    "oversample = SMOTENC(categorical_features=catIndList,random_state=0,sampling_strategy='not majority')\n",
    "x_o,y_o = oversample.fit_resample(sparse_df,df.outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dependent and independent var back together to save to csv\n",
    "over_np = np.column_stack([x_o,y_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating headers to conver oversample data back to dataframe b4 saving to csv\n",
    "coll = list(sparse_df.columns)\n",
    "coll.append('outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save oversampled data to csv\n",
    "pd.DataFrame(over_np,columns=coll).to_csv(\"oversampledTrain.csv\")"
   ]
  },
  {
   "source": [
    "### ------End of Synthertic Data Creation -----------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Begin analysis with Synthetic Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### load in oversample csv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_df = pd.read_csv('/content/drive/MyDrive/DoR/oversampledTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure age is categorical, drop unwanted columns\n",
    "o_df.age = o_df.age.astype('object')\n",
    "o_df.data_labels = o_df.date_labels.astype('object')\n",
    "y_tr = o_df.outcome\n",
    "X_tr = o_df\n",
    "if 'outcome' in X_tr.columns:\n",
    "      X_tr = X_tr.drop('outcome',1)\n",
    "      print(\"dropping the outcome column\")\n",
    "if 'Unnamed: 0' in X_tr.columns:\n",
    "      X_tr = X_tr.drop('Unnamed: 0',1)\n",
    "      print(\"dropping the Unnamed column\")      \n"
   ]
  },
  {
   "source": [
    "## ------------------------------- End of Over Sampling Section------------------------------------------"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = res\n",
    "y = y_tr\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 5, stop = 50, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 24.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [5, 16, 27, 38, 50]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, n_jobs = -1)\n",
    "# Fit the random search model9\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([157.13379264, 280.14043705, 741.09245086, 479.70631162,\n",
       "        528.13843187, 338.64585431, 280.68518726,  85.83966589,\n",
       "        356.48751616, 320.33887943]),\n",
       " 'std_fit_time': array([48.65763395, 26.34444578, 34.5810544 , 27.10991371,  8.2603901 ,\n",
       "        10.46760837, 17.88680982,  5.40086341, 17.04137483,  5.84005381]),\n",
       " 'mean_score_time': array([ 4.35861023, 11.64101807, 18.48631295,  8.00393907, 10.12099997,\n",
       "        12.51503968, 12.80548358,  3.40196125,  3.45836441,  2.49748786]),\n",
       " 'std_score_time': array([0.05164713, 1.01132544, 6.38898783, 3.03023367, 3.60654071,\n",
       "        2.52892186, 3.6641926 , 1.80489655, 1.24466674, 0.2019956 ]),\n",
       " 'param_n_estimators': masked_array(data=[5, 27, 50, 27, 38, 27, 27, 5, 27, 38],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[5, 5, 5, 2, 5, 5, 2, 5, 5, 2],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[1, 4, 4, 2, 4, 4, 2, 4, 2, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'auto', 'auto', 'auto', 'auto', 'auto', 'auto',\n",
       "                    'sqrt', 'sqrt', 'sqrt'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[40, 70, 40, 70, None, 20, 40, 40, 50, 20],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_bootstrap': masked_array(data=[False, True, False, False, False, False, True, False,\n",
       "                    False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_estimators': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_features': 'sqrt',\n",
       "   'max_depth': 40,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 27,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_features': 'auto',\n",
       "   'max_depth': 70,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 50,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_features': 'auto',\n",
       "   'max_depth': 40,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 27,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_features': 'auto',\n",
       "   'max_depth': 70,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 38,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_features': 'auto',\n",
       "   'max_depth': None,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 27,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_features': 'auto',\n",
       "   'max_depth': 20,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 27,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_features': 'auto',\n",
       "   'max_depth': 40,\n",
       "   'bootstrap': True},\n",
       "  {'n_estimators': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'max_depth': 40,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 27,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 2,\n",
       "   'max_features': 'sqrt',\n",
       "   'max_depth': 50,\n",
       "   'bootstrap': False},\n",
       "  {'n_estimators': 38,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'max_depth': 20,\n",
       "   'bootstrap': False}],\n",
       " 'split0_test_score': array([0.62832035, 0.62055151, 0.62363905, 0.62638283, 0.62200778,\n",
       "        0.61762647, 0.62677658, 0.61909524, 0.62713284, 0.61899524]),\n",
       " 'split1_test_score': array([0.62857661, 0.62321404, 0.6240703 , 0.62525157, 0.62447031,\n",
       "        0.61969525, 0.62545157, 0.61928274, 0.62655783, 0.61643271]),\n",
       " 'split2_test_score': array([0.62625549, 0.61972412, 0.62076789, 0.62362419, 0.62033663,\n",
       "        0.61589905, 0.62339294, 0.61797409, 0.62450546, 0.61438027]),\n",
       " 'mean_test_score': array([0.62771748, 0.62116322, 0.62282575, 0.6250862 , 0.62227157,\n",
       "        0.61774025, 0.62520703, 0.61878402, 0.62606538, 0.61660274]),\n",
       " 'std_test_score': array([0.00103906, 0.00148897, 0.00146574, 0.00113226, 0.00169784,\n",
       "        0.00155188, 0.00139215, 0.0005778 , 0.00112773, 0.00188789]),\n",
       " 'rank_test_score': array([ 1,  7,  5,  4,  6,  9,  3,  8,  2, 10])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( rf_random, open( \"rf10overSample\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=40, max_features='sqrt',\n",
       "                       min_samples_split=5, n_estimators=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "randomForestprediction = list(rf_random.best_estimator_.predict(X_test))\n",
    "randomForestConfusionMatrix = confusion_matrix(y_test, randomForestprediction, labels=[\"recovered\",\"nonhospitalized\",\"hospitalized\",\"deceased\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11809,   192, 12821,  5178],\n",
       "       [  316, 29126,    12,   545],\n",
       "       [ 5327,     4, 20061,  4608],\n",
       "       [ 3367,   122, 12152, 14359]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForestConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgBoost = xgb.XGBClassifier(random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifier' object has no attribute 'getparams'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8387891bf950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxgBoost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'XGBClassifier' object has no attribute 'getparams'"
     ]
    }
   ],
   "source": [
    "xgBoost.getparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Learning Rate\n",
    "eta = [double(x) for x in np.linspace(start = 0, stop = 1, num = 0.25)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  5.3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier()\n",
    "param_dist = {'n_estimators': stats.randint(150, 1000),\n",
    "              'learning_rate': stats.uniform(0.01, 0.6),\n",
    "              'subsample': stats.uniform(0.3, 0.9),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],              \n",
    "              'min_child_weight': [1, 2, 3, 4]\n",
    "             }\n",
    "\n",
    "xgb_rs = RandomizedSearchCV(clf_xgb, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = 3,  \n",
    "                         n_iter = 10, \n",
    "                         verbose = 5, \n",
    "                         n_jobs = -1)\n",
    "xgb_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38564bitvenvvenv0b4b7bb6ecc2428c906fb2ce44624bf3",
   "display_name": "Python 3.8.5 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "247f7c4618e76eff7c429aa197a3125858b521fb51aeb6b0f2942bd7114151ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}